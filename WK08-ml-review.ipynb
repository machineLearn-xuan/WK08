{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Flow for Training/Fitting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_utils import RandomForestClassifier, StandardScaler\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from data_utils import object_from_json_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Stages\n",
    "- Data Prep: Encoding, Scaling, Clustering, sometimes Splitting into train/test datasets\n",
    "- Modeling: `fit()` classifier\n",
    "- Evaluation: `predict()` and measure error\n",
    "\n",
    "#### Data Prep:\n",
    "Do we need to split our data, or is it already split into train/test sets?\n",
    "\n",
    "If it's already split we prepare the Encoding, Scaling, Clustering objects using the `train` data (usually with the `fit_transform()` function), and then we use those same objects to encode, scale, cluster the `test` data (usually with the `transform()` function).\n",
    "\n",
    "If the data is not split into two datasets, we could first split it and repeat the steps above, or, although it might add a bit of bias to the models, we could perform encoding, scaling, clustering with `fit_transform()` on the entire dataset and then only split the already encoded, scaled, clustered data. This biases the encoder, scaler, cluster models, and in turn, the model, but is a bit easier to perform.\n",
    "\n",
    "#### Modeling\n",
    "Once we have `train` and `test` datasets that has been encoded, scaled, clustered, we can use the `train` dataset to fit a supervised model (classifier, regression, etc).\n",
    "\n",
    "Here we will usually call a `fit()` function with the training dataset's features and, separately, its labels or outcome variable values. Something like `fit(features, labels)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "We have a model we trained/fitted with the `train` dataset. Now we can measure how well it actually performs once it's used without the correct labels.\n",
    "\n",
    "Here we usually call `predict()` with a dataset's features to get label or regression predictions.\n",
    "\n",
    "We want to call `predict()` for both the `train` and `test` dataset, and then measure how close those predictions are to the actual labels and values that we have in our dataset.\n",
    "\n",
    "Eavluating with the `train` dataset will tell us if the model is capable of learning anything about the data. Evaluating with the `train` dataset will tell us if the model is capable of learning patterns and trends beyond the data that is fed to it.\n",
    "\n",
    "It's common for the model to perform better with the `train` data since it was trained using that data and labels, but the `test` dataset error is what's more important because it will tell us what kind of error to expect from data that the model hasn't seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Classifying penguins based on measurements.\n",
    "\n",
    "Let's load a dataset and look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PENGUIN_URL = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/refs/heads/main/datasets/json/penguins.json\"\n",
    "penguin_data = object_from_json_url(PENGUIN_URL)\n",
    "\n",
    "display(penguin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't have separate train and test data, so we can either \n",
    "\n",
    "#### Pre-process and then split:\n",
    "\n",
    "<img src=\"./imgs/datasplit-00.jpg\" width=\"720px\"/>\n",
    "\n",
    "OR\n",
    "#### Split and then process:\n",
    "\n",
    "<img src=\"./imgs/datasplit-01.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put in DataFrames\n",
    "penguin_df = ''\n",
    "\n",
    "# TODO: Encode Species Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "\n",
    "Using `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with train_test_split()\n",
    "penguin_train, penguin_test = train_test_split(penguin_df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_scaler = StandardScaler()\n",
    "train_scaled_df = penguin_scaler.fit_transform(penguin_train.drop(columns=[\"species\", \"label\"]))\n",
    "\n",
    "train_scaled_df[\"label\"] = penguin_train[\"label\"].values\n",
    "display(train_scaled_df)\n",
    "train_scaled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Test data\n",
    "test_scaled_df = penguin_scaler.transform(penguin_test.drop(columns=[\"species\", \"label\"]))\n",
    "\n",
    "test_scaled_df[\"label\"] = penguin_test[\"label\"].values\n",
    "display(test_scaled_df)\n",
    "test_scaled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/Fit\n",
    "\n",
    "We can train our model now. We're going to use a `RandomForestClassifier` and `fit()` it with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit RandomForestClassifier\n",
    "penguin_model = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "We can now run predictions for both `train` and `test` data and measure error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: predict() train and test\n",
    "train_pred = ''\n",
    "test_pred = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure classification error with classification_error()\n",
    "display(classification_error(train_scaled_df[\"label\"], train_pred))\n",
    "display(classification_error(test_scaled_df[\"label\"], test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Confusion (Matrix)\n",
    "\n",
    "`display_confusion_matrix(labels, predictions, display_labels=unique_labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at confusion matrices\n",
    "display_confusion_matrix(train_scaled_df[\"label\"], train_pred, display_labels=penguin_df[\"species\"].unique().tolist())\n",
    "display_confusion_matrix(test_scaled_df[\"label\"], test_pred, display_labels=penguin_df[\"species\"].unique().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
